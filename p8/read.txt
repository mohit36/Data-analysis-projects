Nice catch, and thanks for laying out both versionsâ€”that makes it super clear whatâ€™s clashing.

Right now you effectively have **two different designs** for `micro_llm.py`:

1. Old one: `confirm/decline/other` (provider-aware, Ollama/OpenAI)
2. New one: `YES/NO/AMBIGUOUS/UNRELATED` (no provider switching)

You want **one final file** that:

* Works with **Ollama OR OpenAI/Azure**, controlled by `.env`
* Exposes **`classify_contact_reply(user_text)` â†’ YES/NO/AMBIGUOUS/UNRELATED**
* And (for safety) still supports `classify_contact_reply_small(...)` if some old code is still calling it

So weâ€™ll merge them.

---

## âœ… Step 1 â€“ Replace `micro_llm.py` with a unified, provider-aware version

**File:** `chatbot_agents/micro_llm.py`
**Action:** Replace the *entire* file content with this:

```python
# chatbot_agents/micro_llm.py
#
# Contact reply classifier with provider switching (Ollama / Azure / OpenAI).
#
# Main API:
#   - classify_contact_reply(user_text) -> "YES" | "NO" | "AMBIGUOUS" | "UNRELATED"
#
# Compatibility API (if older code still uses it):
#   - classify_contact_reply_small(offer_text, reply) -> "confirm" | "decline" | "other"
#
# Strategy:
#   1. Fast heuristic for obvious YES/NO/UNRELATED.
#   2. If result is AMBIGUOUS and provider is configured, call a small LLM
#      via Ollama or Azure/OpenAI (controlled by env).

import os
import re
import logging
from typing import Literal

import httpx
from dotenv import load_dotenv
import openai

load_dotenv()
logger = logging.getLogger(__name__)

# -------------------------------------------------------------------
# Types
# -------------------------------------------------------------------

ContactLabel = Literal["YES", "NO", "AMBIGUOUS", "UNRELATED"]
ReplyDecision = Literal["confirm", "decline", "other"]

# -------------------------------------------------------------------
# Env config
# -------------------------------------------------------------------

# Which micro LLM provider to use for ambiguous cases:
#   - "ollama"  -> call local Ollama
#   - "azure"   -> call Azure OpenAI (engine = MICRO_LLM_DEPLOYMENT)
#   - "openai"  -> call OpenAI model (model = SMALL_LLM_MODEL)
#   - anything else / empty -> heuristic only (no LLM)
SMALL_LLM_PROVIDER = os.getenv("SMALL_LLM_PROVIDER", "ollama").lower()

# Ollama config
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_CONTACT_MODEL = os.getenv("OLLAMA_CONTACT_MODEL", "llama3.2:latest")
USE_OLLAMA_CONTACT = os.getenv("USE_OLLAMA_CONTACT", "true").lower() in {
    "1",
    "true",
    "yes",
}

# Azure / OpenAI config for micro LLM
# For Azure:
#   - CHAT_PROVIDER=azure
#   - AZURE_OPENAI_* already set globally (same as exigotech_chatbot.py)
MICRO_LLM_DEPLOYMENT = os.getenv("MICRO_LLM_DEPLOYMENT") or os.getenv(
    "AZURE_CHAT_DEPLOYMENT"
)
SMALL_LLM_MODEL = os.getenv("SMALL_LLM_MODEL", "gpt-4.1-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


# -------------------------------------------------------------------
# Heuristic classifier
# -------------------------------------------------------------------

def _heuristic_contact_label(text: str) -> ContactLabel:
    """Very cheap classifier for obvious yes/no/unrelated answers."""
    if not text:
        return "AMBIGUOUS"

    t = text.strip().lower()

    # Pure yes-ish
    yes_patterns = [
        r"^y(es)?[!.]*$",
        r"^yeah[!.]*$",
        r"^yep[!.]*$",
        r"^sure[!.]*$",
        r"^ok(ay)?[!.]*$",
        r"^please do.*",
        r".*\bgo ahead\b.*",
        r".*\bconnect (me|us)\b.*",
        r".*\bput me in touch\b.*",
        r".*\bthat would be great\b.*",
    ]

    for pat in yes_patterns:
        if re.search(pat, t):
            return "YES"

    # Pure no-ish
    no_patterns = [
        r"^n(o|ope)[!.]*$",
        r"^nah[!.]*$",
        r"^not now.*",
        r"^maybe later.*",
        r".*\bno thanks\b.*",
        r".*\bno,? that'?s ok\b.*",
        r".*\bi'?ll reach out myself\b.*",
        r".*\bi will contact\b.*",
        r".*\bi already have their details\b.*",
    ]

    for pat in no_patterns:
        if re.search(pat, t):
            return "NO"

    # Very clearly a *new* question, not a yes/no
    question_words = ["what", "how", "where", "when", "who", "which", "why"]
    if "?" in t or any(t.startswith(w + " ") for w in question_words):
        return "UNRELATED"

    # Default: needs LLM help
    return "AMBIGUOUS"


# -------------------------------------------------------------------
# LLM-based refinement (provider-aware)
# -------------------------------------------------------------------

async def _call_ollama_contact_label(prompt: str) -> ContactLabel:
    """Call Ollama /api/chat and interpret its one-word reply."""
    if not USE_OLLAMA_CONTACT:
        return "AMBIGUOUS"

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            resp = await client.post(
                f"{OLLAMA_BASE_URL}/api/chat",
                json={
                    "model": OLLAMA_CONTACT_MODEL,
                    "messages": [
                        {"role": "user", "content": prompt},
                    ],
                },
            )
            resp.raise_for_status()
            data = resp.json()
            content = (
                data.get("message", {})
                .get("content", "")
                .strip()
                .upper()
            )
    except Exception as e:
        logger.warning("Ollama contact classifier failed: %s", e)
        return "AMBIGUOUS"

    if "YES" in content:
        return "YES"
    if "NO" in content and "YES" not in content:
        return "NO"
    if "UNRELATED" in content:
        return "UNRELATED"
    if "AMBIGUOUS" in content:
        return "AMBIGUOUS"

    return "AMBIGUOUS"


async def _call_openai_contact_label(prompt: str) -> ContactLabel:
    """
    Use Azure/OpenAI small model to classify the reply.
    For Azure, ensure openai.api_type/api_base/api_key are set globally.
    """
    if not OPENAI_API_KEY and SMALL_LLM_PROVIDER == "openai":
        logger.warning(
            "OPENAI_API_KEY not set for SMALL_LLM_PROVIDER=openai; "
            "falling back to heuristic only."
        )
        return "AMBIGUOUS"

    try:
        if SMALL_LLM_PROVIDER == "azure":
            if not MICRO_LLM_DEPLOYMENT:
                logger.warning(
                    "MICRO_LLM_DEPLOYMENT not set for SMALL_LLM_PROVIDER=azure."
                )
                return "AMBIGUOUS"

            resp = await openai.ChatCompletion.acreate(
                engine=MICRO_LLM_DEPLOYMENT,
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=5,
            )
        else:  # "openai"
            resp = await openai.ChatCompletion.acreate(
                model=SMALL_LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=5,
            )

        raw = resp["choices"][0]["message"]["content"].strip().upper()
    except Exception as e:
        logger.warning("OpenAI/Azure contact classifier failed: %s", e)
        return "AMBIGUOUS"

    if "YES" in raw:
        return "YES"
    if "NO" in raw and "YES" not in raw:
        return "NO"
    if "UNRELATED" in raw:
        return "UNRELATED"
    if "AMBIGUOUS" in raw:
        return "AMBIGUOUS"

    return "AMBIGUOUS"


async def _llm_contact_label(text: str) -> ContactLabel:
    """Dispatch to the appropriate provider based on SMALL_LLM_PROVIDER."""
    if SMALL_LLM_PROVIDER == "ollama":
        prompt = (
            "You are a classifier for a chatbot contact flow.\n"
            "The assistant previously asked the user if they want to be connected "
            "to a sales/contact person. The user replied:\n\n"
            f'"{text}"\n\n'
            "Classify the user reply into exactly ONE of these labels:\n"
            "- YES: user clearly wants to be connected / share contact details.\n"
            "- NO: user clearly does NOT want contact now.\n"
            "- AMBIGUOUS: unclear, hedging, or mixed.\n"
            "- UNRELATED: reply is a new question or unrelated to the contact offer.\n\n"
            "Reply with ONLY the label word: YES, NO, AMBIGUOUS, or UNRELATED."
        )
        return await _call_ollama_contact_label(prompt)

    if SMALL_LLM_PROVIDER in ("azure", "openai"):
        prompt = (
            "You are a classifier for a chatbot contact flow.\n"
            "The assistant previously asked the user if they want to be connected "
            "to a sales/contact person. The user replied:\n\n"
            f'"{text}"\n\n'
            "Classify the user reply into exactly ONE of these labels:\n"
            "- YES: user clearly wants to be connected / share contact details.\n"
            "- NO: user clearly does NOT want contact now.\n"
            "- AMBIGUOUS: unclear, hedging, or mixed.\n"
            "- UNRELATED: reply is a new question or unrelated to the contact offer.\n\n"
            "Reply with ONLY the label word: YES, NO, AMBIGUOUS, or UNRELATED."
        )
        return await _call_openai_contact_label(prompt)

    # Unknown provider or disabled => no LLM refinement
    return "AMBIGUOUS"


# -------------------------------------------------------------------
# Public APIs
# -------------------------------------------------------------------

async def classify_contact_reply(user_text: str) -> ContactLabel:
    """
    Main API:
      - Run fast heuristic first.
      - Only if AMBIGUOUS and provider configured, optionally call a small LLM.
    """
    label = _heuristic_contact_label(user_text)
    if label != "AMBIGUOUS":
        return label

    try:
        return await _llm_contact_label(user_text)
    except Exception as e:
        logger.warning("classify_contact_reply LLM path failed: %s", e)
        # Fail safe: don't break the flow just because LLM failed.
        return "AMBIGUOUS"


# Compatibility layer for older "confirm/decline/other" code, if any:
async def classify_contact_reply_small(
    offer_text: str,
    reply: str,
) -> ReplyDecision:
    """
    Map the richer YES/NO/AMBIGUOUS/UNRELATED labels to
    confirm/decline/other for older call sites, if still used.
    """
    label = await classify_contact_reply(reply)

    if label == "YES":
        return "confirm"
    if label == "NO":
        return "decline"

    # For AMBIGUOUS or UNRELATED, treat as 'other'
    return "other"
```

This file now:

* Supports **Ollama OR Azure OR OpenAI**, chosen via `.env`
* Exposes **`classify_contact_reply`** (new logic, YES/NO/AMBIGUOUS/UNRELATED)
* Exposes **`classify_contact_reply_small`** (legacy confirm/decline/other mapping)
* Uses your existing global OpenAI/Azure config (same as in `exigotech_chatbot.py`)

---

## âœ… Step 2 â€“ How to configure providers in `.env`

You can now switch behavior with **only env changes**, no code edits:

### ðŸ”¹ Use **Ollama** (local, free)

```env
SMALL_LLM_PROVIDER=ollama
USE_OLLAMA_CONTACT=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_CONTACT_MODEL=llama3.2:latest
```

No `OPENAI_API_KEY` needed for this.

---

### ðŸ”¹ Use **Azure** (cheap small deployment)

```env
SMALL_LLM_PROVIDER=azure
MICRO_LLM_DEPLOYMENT=gpt-4o-mini        # or any cheap Azure deployment name
# AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, etc., already used by your main code
```

This reuses your global Azure OpenAI config set in `exigotech_chatbot.py`.

---

### ðŸ”¹ Use **OpenAI** (direct API, non-Azure)

```env
SMALL_LLM_PROVIDER=openai
OPENAI_API_KEY=sk-xxxx
SMALL_LLM_MODEL=gpt-4.1-mini            # or gpt-4o-mini, etc.
```

For this case, make sure you *donâ€™t* override `openai.api_type="azure"` globally, or youâ€™ll confuse it. If needed, we can later separate Azure vs OpenAI clients, but for now Iâ€™d recommend **Ollama or Azure** as the two main modes.

---

## âœ… Step 3 â€“ Make sure imports match

Given what we did earlier, make sure in `chatbot_api.py` you have:

```python
from chatbot_agents.micro_llm import classify_contact_reply
```

If you still have any old code that imports `classify_contact_reply_small`, it will still work because we kept that function as a wrapper.

---

## âœ… Step 4 â€“ Sanity check

After replacing `micro_llm.py` and checking imports:

1. Restart your backend.
2. Hit an endpoint that triggers contact offer:

   * Ask about a service,
   * Bot offers â€œDo you want me to connectâ€¦â€
   * Reply with:

     * â€œyesâ€
     * â€œno thanksâ€
     * â€œmaybe laterâ€
     * â€œwhat are your cloud services?â€
3. Watch logs for lines like:

```text
Contact follow-up detected. User reply='yes' -> label=YES
Contact follow-up detected. User reply='no thanks' -> label=NO
...
```

If anything throws an error, paste the traceback and Iâ€™ll patch that **without** rewriting the whole file again.

Once this micro_llm layer is stable, we can go back to:

* The YES branch doing RAG + contact answer + session reset
* Verify that the flow actually ends the session after contact
* Then final clean-up of prompts so they produce EXACT, predictable phrasing for `looks_like_contact_offer`.
