import os
import logging
import json
import asyncio
import numpy as np
from typing import List, Dict, Tuple, Optional, Any
from sentence_transformers import SentenceTransformer, CrossEncoder
from functools import lru_cache
from tenacity import retry, stop_after_attempt, wait_fixed
import openai

# Import Database Helpers
from pgvector_helper.pgvector_client_connection import get_pgvector_client
from pgvector_helper.get_location_by_url import get_collection_by_url
from pgvector_helper.db_logger import get_db_logger
from dotenv import load_dotenv

load_dotenv()
logger = get_db_logger(__name__)

# --- GLOBAL CONFIGURATION (Load Models ONCE) ---
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "thenlper/gte-large")
RERANK_MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"
DEPLOYMENT_NAME = os.getenv("AZURE_CHAT_DEPLOYMENT", "gpt-4-1-mini")

# Initialize Models Globally (Singleton Pattern)
# This prevents reloading the model on every single search request (Huge Latency Fix)
logger.info("‚è≥ Loading Embedding Model...")
embed_model = SentenceTransformer(EMBEDDING_MODEL)
logger.info("‚úÖ Embedding Model Loaded.")

try:
    logger.info("‚è≥ Loading Reranker Model...")
    rerank_model = CrossEncoder(RERANK_MODEL_NAME)
    logger.info("‚úÖ Reranker Model Loaded.")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è CrossEncoder failed to load: {e}. Reranking will be skipped.")
    rerank_model = None

# OpenAI Config
openai.api_type = "azure"
openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
openai.api_version = os.getenv("AZURE_CHAT_API_VERSION", "2024-02-15-preview")


# --- CORE SEARCH FUNCTION (OPTIMIZED SQL) ---
async def async_search_local_pgvector(
    client_conn, 
    query_vector: List[float], 
    top_k: int = 10, 
    source_type: Optional[str] = None, 
    collection_name: Optional[str] = None,
    use_reranking: bool = True
) -> List[Dict[str, Any]]:
    
    if not collection_name:
        logger.error("No collection name provided for search")
        return []

    # loop = asyncio.get_running_loop() # Not needed if using direct execute or connection pool
    
    # OPTIMIZATION: Use Negative Inner Product (<#>) instead of Cosine Distance (<=>)
    # This is faster for normalized vectors (like gte-large).
    # We filter by source_type DIRECTLY in the SQL for speed.
    
    where_clause = ""
    params = [np.array(query_vector), top_k]
    
    if source_type:
        where_clause = "WHERE source_type = %s"
        params.insert(1, source_type) # Insert before top_k
        
    base_sql = f"""
        SELECT url, page_title, chunk_index, text, source_type,
               (vector <#> %s::vector) * -1 AS score
        FROM {collection_name}
        {where_clause}
        ORDER BY vector <#> %s::vector ASC
        LIMIT %s;
    """
    
    # We need to pass the vector twice (once for select score, once for ordering)
    # Params structure: [vector, source_type (optional), vector, top_k]
    sql_params = [np.array(query_vector)]
    if source_type:
        sql_params.append(source_type)
    sql_params.append(np.array(query_vector))
    sql_params.append(top_k)

    try:
        # Use a cursor to execute
        with client_conn.cursor() as cur:
            cur.execute(base_sql, tuple(sql_params))
            rows = cur.fetchall()
            
        # Convert rows to dicts
        results = []
        for row in rows:
            # Row structure depends on DB driver, assuming tuple or dict-like
            # Adjust indices if your driver returns strict tuples
            results.append({
                "url": row[0],
                "page_title": row[1],
                "chunk_index": row[2],
                "text": row[3],
                "source_type": row[4],
                "score": float(row[5])
            })

        # Apply Reranking if enabled and results exist
        if use_reranking and results and rerank_model:
            # We don't need to pass 'query_text' here if we handle it in the reranker wrapper
            # But the wrapper below needs the text. 
            pass # The wrapper is called *outside* or we call it here.
            # Your original code called it outside or mixed it. 
            # We will return raw results here and let the agent handle reranking.
            
        return results

    except Exception as e:
        logger.error(f"Search failed: {e}")
        return []

# --- RERANKING HELPER (GLOBAL MODEL) ---
async def rerank_results_with_cross_encoder(query: str, results: List[Dict], top_k: int = 10):
    if not results or not rerank_model:
        return results

    try:
        # Prepare pairs [Query, Doc Text]
        pairs = [[query, r["text"]] for r in results]
        
        # Predict scores (Sync operation, but fast on CPU for small batches)
        # Using a thread to prevent blocking the event loop
        cross_scores = await asyncio.to_thread(rerank_model.predict, pairs)

        # Update scores
        for idx, result in enumerate(results):
            # Combine scores (Weighted)
            # Vector score is usually 0.7-0.9, Cross score is -10 to 10 (logits) or 0-1 (sigmoid)
            # Defaulting to pure Cross Encoder score for sorting as it's more accurate
            result["cross_score"] = float(cross_scores[idx])
            result["score"] = result["cross_score"] # Override for sorting

        # Sort by new score
        reranked = sorted(results, key=lambda x: x["score"], reverse=True)
        return reranked[:top_k]

    except Exception as e:
        logger.error(f"Reranking failed: {e}")
        return results


# --- SEARCH AGENT CLASS ---
class SearchAgent:
    def __init__(self, collection_name=None):
        self.collection_name = collection_name
        # Assume connection is handled by your helper or passed in
        # For this refactor, we grab a fresh client per request or use a pool
        self.client_conn = get_pgvector_client()

    async def classify_query_type(self, query: str):
        """
        Uses LLM to determine intent (about, contact, etc.)
        """
        try:
            # Your existing prompt logic for intent classification
            # Simplified for brevity - assume 'llm_intent' helper exists or inline it
            from chatbot_agents.chatbot_search_new import llm_intent # Self-import or move helper
            return await llm_intent(query)
        except Exception as e:
            logger.error(f"Intent classification failed: {e}")
            return "page"

    async def search_async(self, query: str, source_type: str = None, top_k: int = 10, country_code: str = None, request_url: str = None):
        """
        Main entry point for search.
        1. Embed Query
        2. Determine Collection (Geo-Routing)
        3. Search DB
        4. Rerank
        """
        # 1. Embed Query
        query_vector = embed_model.encode([query])[0].tolist()

        # 2. Determine Collection
        # If country_code provided, try to route. Otherwise default.
        final_collection = self.collection_name
        if not final_collection:
            # Use your helper to find collection based on URL/Country
            # final_collection = get_collection_by_url(...) 
            # Hardcoding fallback for now based on your env
            final_collection = os.getenv("PGTABLE_BASE", "website_chunks_ultra_v1")
            if country_code:
                final_collection = f"{final_collection}_{country_code.lower()}"

        # 3. DB Search
        results = await async_search_local_pgvector(
            self.client_conn,
            query_vector,
            top_k=top_k * 2, # Fetch more for reranking
            source_type=source_type,
            collection_name=final_collection,
            use_reranking=False # We handle it manually below
        )

        # 4. Rerank
        final_results = await rerank_results_with_cross_encoder(query, results, top_k=top_k)
        
        return final_results

# --- LLM INTENT HELPER (Kept from your code) ---
async def llm_intent(query: str):
    # ... (Your existing OpenAI intent logic here) ...
    # Ensure you copy your 'llm_intent' function logic here
    # I am omitting the prompt text to save space, but paste your original function here.
    # It was logically fine, just needs to return the string.
    pass
async def async_search_local_pgvector(
    client_conn, 
    query_vector: List[float], 
    top_k: int = 10, 
    source_type: Optional[str] = None, 
    collection_name: Optional[str] = None,
    use_reranking: bool = True
) -> List[Dict[str, Any]]:
    
    if not collection_name:
        logger.error("No collection name provided for search")
        return []

    # 1. Define the Search Logic (Reusable)
    def build_query(target_source: Optional[str]):
        where_clause = ""
        params = [np.array(query_vector), top_k]
        
        # Use simple WHERE for filtering
        if target_source:
            where_clause = "WHERE source_type = %s"
            params.insert(1, target_source)
            
        # REVERT TO COSINE DISTANCE (<=>)
        # It is robust, works with standard indexes, and avoids negative math confusion.
        sql = f"""
            SELECT url, page_title, chunk_index, text, source_type,
                   (vector <=> %s::vector) as distance
            FROM {collection_name}
            {where_clause}
            ORDER BY vector <=> %s::vector ASC
            LIMIT %s;
        """
        # Params: [source (opt), vector, vector, top_k]
        sql_params = []
        if target_source:
            sql_params.append(target_source)
        sql_params.append(np.array(query_vector)) # For SELECT
        sql_params.append(np.array(query_vector)) # For ORDER BY
        sql_params.append(top_k)
        
        return sql, tuple(sql_params)

    try:
        results = []
        
        # 2. ATTEMPT 1: Strict Filter (If intent is provided)
        if source_type and source_type != "page": # Don't filter if intent is just 'page'
            sql, params = build_query(source_type)
            with client_conn.cursor() as cur:
                cur.execute(sql, params)
                rows = cur.fetchall()
            
            if rows:
                logger.info(f"üéØ Strict Search ({source_type}) found {len(rows)} results.")
                # Process rows...
                results = [
                    {
                        "url": r[0], "page_title": r[1], "chunk_index": r[2], 
                        "text": r[3], "source_type": r[4], 
                        # Convert Distance (0..2) to Similarity (1..0) for Reranker
                        "score": 1 - float(r[5]) 
                    } for r in rows
                ]
                return results
            else:
                logger.warning(f"‚ö†Ô∏è Strict Search ({source_type}) found NOTHING. Falling back to broad search.")

        # 3. ATTEMPT 2: Broad Search (Fallback)
        # If strict search failed, or no source_type was given, search EVERYTHING.
        sql, params = build_query(None)
        with client_conn.cursor() as cur:
            cur.execute(sql, params)
            rows = cur.fetchall()

        results = [
            {
                "url": r[0], "page_title": r[1], "chunk_index": r[2], 
                "text": r[3], "source_type": r[4], 
                "score": 1 - float(r[5]) 
            } for r in rows
        ]
        
        return results

    except Exception as e:
        logger.error(f"Search failed: {e}")
        return []
