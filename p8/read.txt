def _normalize_reply(s: str) -> str:
    return s.strip().lower()


def fast_reply_classifier(text: str) -> str:
    """
    Cheap rule-based classifier for replies to a contact offer.
    Returns: 'confirm', 'decline', or 'ambiguous'.
    """
    q = _normalize_reply(text)

    confirm_strict = {
        "yes",
        "y",
        "yeah",
        "yep",
        "sure",
        "ok",
        "okay",
        "please do",
    }
    decline_strict = {
        "no",
        "n",
        "nope",
        "not now",
        "never mind",
        "cancel",
        "no thanks",
        "no thank you",
    }

    if q in confirm_strict:
        return "confirm"
    if q in decline_strict:
        return "decline"
    return "ambiguous"


async def llm_contact_reply_classifier(offer_text: str, reply: str) -> str:
    """
    Stub for small-LLM-based classification of ambiguous replies.

    Returns: 'confirm', 'decline', or 'other'.

    For now this is a placeholder that always returns 'other'.
    Later we will implement this using Ollama or a cheap GPT-mini model.
    """
    # TODO: implement with a small model (Ollama or gpt-4.1-mini)
    return "other"


Handle pending contact offer at the top of summarize

Now, inside summarize, right after you get sid, msg_count, and state, we add logic that intercepts â€œyes/noâ€ before RAG.

Find this block:

        # 2) DB-based counters (used for potential business heuristics)
        msg_count = await get_msg_count(sid)
        state = get_session_state(sid)
        business_count = int(state.get("business_count", 0))

        # 3) Lookup mapped question (from previous related-services buttons)
        mapped_question = lookup_mapped_question(sid, query)


Replace it with this extended block:

        # 2) DB-based counters (used for potential business heuristics)
        msg_count = await get_msg_count(sid)
        state = get_session_state(sid)
        business_count = int(state.get("business_count", 0))

        # 2a) Handle replies to a pending contact offer BEFORE RAG
        pending_offer = state.get("pending_contact_offer")
        if pending_offer:
            decision = fast_reply_classifier(query)

            if decision == "ambiguous":
                # Escalate to small LLM later (stub for now)
                decision = await llm_contact_reply_classifier(
                    pending_offer.get("offer_text", ""),
                    query,
                )

            if decision == "confirm":
                # User clearly agreed to contact
                state["pending_contact_offer"] = None

                confirm_msg = (
                    f"Great! I'll connect you with our team about "
                    f"{pending_offer.get('service_name', 'this service')}.\n\n"
                    "Please share your email and phone number so our team can reach out."
                )

                await push_user(sid, query)
                await push_assistant_sanitized(sid, confirm_msg)

                return {
                    "summary": confirm_msg,
                    "useful_links": [],
                    "mapped_questions": {},
                    "needs_clarification": False,
                    "inside_contact_flow": True,
                    "needs_contact_details": True,
                }

            if decision == "decline":
                # User declined the offer
                state["pending_contact_offer"] = None

                decline_msg = (
                    "No worries, I wonâ€™t connect you right now. "
                    "You can continue exploring services or ask anything else."
                )

                await push_user(sid, query)
                await push_assistant_sanitized(sid, decline_msg)

                return {
                    "summary": decline_msg,
                    "useful_links": [],
                    "mapped_questions": {},
                    "needs_clarification": False,
                    "inside_contact_flow": False,
                }

            # decision == 'other' â†’ user moved on; drop offer and continue normally
            state["pending_contact_offer"] = None

        # 3) Lookup mapped question (from previous related-services buttons)
        mapped_question = lookup_mapped_question(sid, query)


So now:

If a contact offer was active,

and user sends â€œyesâ€ / â€œnoâ€ / â€œmaybe yes-ishâ€,

we handle that separately and do not send it into RAG.

For now ambiguous replies are treated as "other" (no special handling).
Later weâ€™ll wire the small LLM so "ambiguous" can become "confirm" or "decline".

3ï¸âƒ£ Set pending_contact_offer when we inject contact button

Scroll down to step 10 in summarize, where we inject the contact button:

        # 10) Optional contact-button injection (still just label -> question)
        if isinstance(parsed_json, dict):
            lowered_query = (processed_query or "").lower()
            business_keywords = (
                "price",
                "pricing",
                "license",
                "licence",
                "quote",
                "demo",
                "cost",
                "sales",
                "buy",
            )
            is_business_like = any(k in lowered_query for k in business_keywords)

            intent_str = (
                query_intent.lower()
                if isinstance(query_intent, str)
                else ""
            )

            should_inject_contact = is_business_like or intent_str in ("contact",)

            if should_inject_contact and "contact" not in parsed_json:
                # Try to grab a main service name from existing buttons
                service_names = list(parsed_json.keys())
                if service_names:
                    main_service = service_names[0]
                else:
                    main_service = (
                        processed_query[:80]
                        if processed_query
                        else "Exigotech services"
                    )

                contact_label = f"Contact about {main_service}"
                contact_question = (
                    f"How can I contact Exigotech sales team about {main_service}?"
                )

                parsed_json[contact_label] = contact_question


ğŸ‘‰ We keep all of this, and add one tiny block at the end of the if should_inject_contact...:

                parsed_json[contact_label] = contact_question

                # Mark a pending contact offer for the *next* user reply
                state["pending_contact_offer"] = {
                    "service_name": main_service,
                    "offer_text": contact_question,
                }


So now, whenever we inject a â€œContact about Xâ€ button, we also record in session:

state["pending_contact_offer"] = {
    "service_name": "...",
    "offer_text": "How can I contact Exigotech sales team about ...?"
}


On the next user message:

If they just click the button â†’ the UI will send the mapped question (good, still goes through RAG).

If they type â€œyesâ€ / â€œnoâ€ / â€œyes pleaseâ€ â†’ our new layer intercepts it and handles as contact reaction.

4ï¸âƒ£ Whatâ€™s working now (even before small LLM is added)

After these edits:

âœ… Contact buttons still work as before (mapped_questions & mapping store unchanged).

âœ… A â€œContact about Xâ€ injection now also sets pending_contact_offer.

âœ… If the bot shows a contact CTA and the user replies with:

â€œyesâ€ / â€œyeahâ€ / â€œokâ€ / â€œnoâ€ / â€œnopeâ€ / â€œnot nowâ€,
â†’ handled without going into RAG.

âœ… For weird replies (â€œyes but laterâ€), we currently treat them as â€œotherâ€ and just drop the pending offer, then continue normally.

ğŸš§ Later, when we plug in Ollama or GPT-mini inside llm_contact_reply_classifier, those ambiguous cases will be upgraded to smart â€œconfirm/declineâ€ classification.

Nothing else in your pipeline changes.
No prompts changed yet.
No .env changes needed for this step.

5ï¸âƒ£ Next steps (after you paste this and it runs)

Once this compiles and your API runs:

Weâ€™ll create a tiny micro_llm.py with an actual cheap_llm_call() that talks to Ollama (or GPT-mini).

Weâ€™ll replace the stub in llm_contact_reply_classifier with a real implementation.

After that, weâ€™ll move on to generate_exigotech_answer and then finally to prompt engineering.

If you run into any error after pasting these blocks (typo, indent, whatever), copy the traceback or the failing snippet and Iâ€™ll debug it with you.



