Nice, respect for actually wiring it the way we discussed üôå
Since search_internal_answers_async is now in the right shape, next step is:

Plug it into agent_search_async and add a clean merge + single rerank.

Let‚Äôs do that in 2 small, focused moves.

1Ô∏è‚É£ Add a helper to merge + dedup candidates

Put this somewhere below SearchAgent in chatbot_search_new.py (or as a @staticmethod inside the class if you prefer).

from typing import Any, Dict, List

def merge_and_dedup_candidates(*result_lists: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Merge candidates from multiple sources and deduplicate by URL or internal id.

    Expected fields:
      - url (for website chunks) OR
      - id + source_type == "internal" (for internal answers)
    """

    all_results: List[Dict[str, Any]] = []
    for lst in result_lists:
        if lst:
            all_results.extend(lst)

    if not all_results:
        return []

    dedup: Dict[str, Dict[str, Any]] = {}

    for r in all_results:
        # Website chunks have a real URL
        if r.get("url"):
            key = r["url"]
        else:
            # Internal answers: use DB id or question as stable key
            internal_id = r.get("id") or r.get("page_title") or r.get("question")
            key = f"internal://{internal_id}"

        score = float(r.get("score", 0.0))

        # Keep the highest-scoring candidate per key
        if key not in dedup or score > dedup[key].get("score", 0.0):
            dedup[key] = r

    # Sort by vector score before cross-encoder rerank (optional but nice)
    return sorted(dedup.values(), key=lambda x: x.get("score", 0.0), reverse=True)


This assumes exactly what you now return from:

async_search_local_pgvector (website)

search_internal_answers_async (internal)

2Ô∏è‚É£ Update agent_search_async to use website + internal + rerank

Now replace the body of SearchAgent.agent_search_async with this version.

Don‚Äôt touch the signature; just replace inside the function.

class SearchAgent:
    def __init__(self, collection_name=None, intent=None):
        # Initialize the agent with a default collection name and other configurations
        self.collection_name = collection_name or PGTABLE_BASE
        self.intent = intent
        self.client_conn = get_pgvector_client()  # Assuming this connects to a database or vector store
        register_vector(self.client_conn)         # Assuming this registers the vector-based client

    async def classify_query_type(self, query):
        try:
            llm_result = await llm_intent(query)
            self.intent = llm_result
            return llm_result
        except Exception as e:
            logger.error(f"Intent classification failed: {e}")
            return "page"  # Default fallback

    async def agent_search_async(self, query, top_k=10, request_url=None, country_code=None):
        """
        Search with intent classification, website search, internal answers search,
        and a single global rerank.
        """

        # 1. Get intent and map to primary source types
        intent = await self.classify_query_type(query)
        logger.info(f"üß† Query intent: {intent}")

        intent_to_source_type = {
            "case": "case",
            "greeting": None,
            "conclusion": None,
            "contact": "contact",
            "about": "about",
            "career": "career",
            "page": "page",
        }
        primary_source_type = intent_to_source_type.get(intent, "page")

        # 2. Resolve table by URL/country (SG/PH vs AU etc.)
        final_table = get_collection_by_url(
            request_url=request_url,
            base_collection=self.collection_name,
            country_code=country_code,
        )

        if not final_table:
            logger.error(f"‚ùå No matching collection (url={request_url}, cc={country_code})")
            raise RuntimeError("No matching regional collection available")

        # 3. Embed query once
        query_vector = model.encode([query])[0].tolist()

        # Oversample for reranking
        top_k_primary = max(top_k * 3, 30)
        top_k_internal = max(top_k, 10)

        # 4. Kick off website + internal search in parallel
        primary_task = async_search_local_pgvector(
            self.client_conn,
            query_vector=query_vector,
            top_k=top_k_primary,
            source_type=primary_source_type,
            collection_name=final_table,
            query_text=query,
            use_reranking=False,  # retrieval only; rerank later
        )

        internal_task = search_internal_answers_async(
            self.client_conn,
            query_text=query,
            query_vector=query_vector,
            top_k=top_k_internal,
            use_reranking=False,  # same idea: retrieval only
        )

        primary_results, internal_results = await asyncio.gather(
            primary_task,
            internal_task,
        )

        logger.info(
            f"üîé Retrieved candidates: website={len(primary_results)}, "
            f"internal={len(internal_results)}"
        )

        # 5. Merge + dedup candidates
        candidates = merge_and_dedup_candidates(primary_results, internal_results)

        if not candidates:
            logger.warning("‚ö†Ô∏è No candidates from website or internal answers")
            return []

        # 6. Single global rerank with cross-encoder
        reranked_results, quality = await rerank_results_with_cross_encoder(
            query=query,
            results=candidates,
            top_k=top_k,
        )

        logger.info(f"üèÅ Final rerank quality: {quality}, returned={len(reranked_results)}")

        return reranked_results

What this gives you right now

‚úÖ Website search (filtered by source_type from intent)

‚úÖ Internal answers search (your cached Q/A DB)

‚úÖ Both run in parallel using asyncio.gather

‚úÖ Results are merged + deduped

‚úÖ One global rerank for all candidates

No general (no-source_type) search yet ‚Äî we can add that in the next step once this is working and tested.

3Ô∏è‚É£ What to do after this compiles

Run a few manual tests in your API:

Normal website-only query (e.g. ‚Äúcloud security services‚Äù).

Pure internal answer query (something you put in cached_question_answer).

A query that could match both.

Check logs for:

Retrieved candidates: website=X, internal=Y

Final rerank quality: ...

If that all looks sane and you‚Äôre not hitting any key errors, we‚Äôre ready for the next enhancement:

Add general (no-source_type) website search as a third parallel task
and include it in merge_and_dedup_candidates ‚Üí reranker.

When you‚Äôre done implementing this step, tell me:

‚ÄúDone, add general search now‚Äù

and I‚Äôll show you the minimal changes (one more task + one more argument into merge_and_dedup_candidates).




Nice, this is a clean piece to add.
Let‚Äôs set up your internal cached Q/A table in Postgres with pgvector + HNSW using cosine distance (matching your query).

I‚Äôll give you:

CREATE EXTENSION (pgvector)

CREATE TABLE cached_question_answer

HNSW index for cosine distance

Optional tuning tips

The HNSW syntax is straight from the official pgvector docs. 
GitHub
+1

1Ô∏è‚É£ Enable pgvector (once per database)

Run this in your DB (as superuser / migration):

CREATE EXTENSION IF NOT EXISTS vector;

2Ô∏è‚É£ Create the internal answers table

You‚Äôre using thenlper/gte-large as the embedding model, which is 1024-dimensional.
If you ever switch models, adjust the 1024 accordingly.

CREATE TABLE cached_question_answer (
    id           bigserial PRIMARY KEY,
    question     text        NOT NULL,
    answer_text  text        NOT NULL,
    frequency    integer     NOT NULL DEFAULT 0,
    embedding    vector(1024) NOT NULL  -- adjust dim if your model is different
);


frequency ‚Üí how often this Q/A is used (for analytics / ordering)

embedding ‚Üí the vector for question (or question+context)

This matches how pgvector expects the vector type to be created. 
GitHub

3Ô∏è‚É£ Create HNSW index for cosine distance

You‚Äôre already querying with:

ORDER BY embedding <=> %s::vector


and comments say ‚Äúcosine distance‚Äù, so we use vector_cosine_ops in the index. 
GitHub
+1

CREATE INDEX cached_question_answer_embedding_hnsw_idx
ON cached_question_answer
USING hnsw (embedding vector_cosine_ops);


If you want explicit params (optional, defaults are usually fine):

CREATE INDEX cached_question_answer_embedding_hnsw_idx
ON cached_question_answer
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

4Ô∏è‚É£ Basic query pattern (what your Python is doing)

This is basically what your function runs:

SELECT
    id,
    question,
    answer_text,
    frequency,
    1 - (embedding <=> $1::vector) AS score
FROM cached_question_answer
ORDER BY embedding <=> $1::vector
LIMIT $2;


embedding <=> $1 is the cosine distance

1 - distance ‚Üí similarity score in [0,1] for your Python dicts

This pattern is exactly how pgvector docs show using HNSW with cosine distance. 
GitHub
+1

5Ô∏è‚É£ Optional: ANALYZE + runtime tuning

After you bulk insert your top 100 Q/As:

ANALYZE cached_question_answer;


If you ever want better recall at the cost of speed, at session level:

SET hnsw.ef_search = 100;  -- default 40, higher = better recall, slower


(You can also SET LOCAL hnsw.ef_search = 100 inside a transaction for one query.) 
GitHub

If you paste your current Python base_sql for search_internal_answers_async I can double-check that the column name (embedding) and dimension (vector(1024)) line up perfectly with this DDL so you don‚Äôt hit any ‚Äúcolumn does not have dimensions‚Äù or type mismatch errors.
