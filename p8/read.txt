class SearchAgent:
    def __init__(self, collection_name=None):
        self.collection_name = collection_name
        self.client_conn = get_pgvector_client()
        register_vector(self.client_conn)

    async def classify_query_type(self, query: str) -> str:
        try:
            return await llm_intent(query)
        except Exception as e:
            logger.exception(f"Intent classification failed: {e}")
            return "page"  # safe default

    async def agent_search_async(
        self,
        query: str,
        top_k: int = 10,
        request_url: str | None = None,
        country_code: str | None = None,
    ) -> list[dict]:
        # 1. Intent â†’ source types
        intent = await self.classify_query_type(query)
        logger.info(f"ðŸ§  Query intent: {intent}")

        primary_source_types = INTENT_TO_SOURCE_TYPES.get(intent, ["page", "blog", "case"])

        # 2. Resolve region-specific table
        final_table = get_collection_by_url(
            request_url=request_url,
            country_code=country_code,
            default_collection=self.collection_name,
        )
        if not final_table:
            logger.error("âŒ No collection table resolved, aborting search")
            return []

        # 3. Embed query once
        query_vector = model.encode(query).tolist()

        # 4. Kick off parallel searches (typed + general + internal)
        top_k_primary = max(top_k * 3, 30)   # oversample for rerank
        top_k_general = max(top_k * 2, 20)
        top_k_internal = top_k              # usually small

        primary_task = async_search_local_pgvector(
            client_conn=self.client_conn,
            query_vector=query_vector,
            top_k=top_k_primary,
            source_type=primary_source_types,
            collection_name=final_table,
            query_text=query,
            use_reranking=False,   # retrieval only
        )

        general_task = async_search_local_pgvector(
            client_conn=self.client_conn,
            query_vector=query_vector,
            top_k=top_k_general,
            source_type=None,      # no filter
            collection_name=final_table,
            query_text=query,
            use_reranking=False,
        )

        internal_task = search_internal_answers_async(
            query_text=query,
            query_vector=query_vector,
            top_k=top_k_internal,
        )

        primary_results, general_results, internal_results = await asyncio.gather(
            primary_task,
            general_task,
            internal_task,
        )

        logger.info(
            f"ðŸ”Ž Retrieved: primary={len(primary_results)}, "
            f"general={len(general_results)}, internal={len(internal_results)}"
        )

        # 5. Merge + dedup candidates
        candidates = self._merge_and_dedup_candidates(
            primary_results,
            general_results,
            internal_results,
        )

        if not candidates:
            logger.warning("âš ï¸ No candidates from any source")
            return []

        # 6. Single global rerank
        final_results, quality = await rerank_results_with_cross_encoder(
            query=query,
            results=candidates,
            top_k=top_k,
        )

        logger.info(f"ðŸ Final rerank quality: {quality}, returned={len(final_results)}")
        return final_results

    def _merge_and_dedup_candidates(
        self,
        primary_results: list[dict],
        general_results: list[dict],
        internal_results: list[dict],
    ) -> list[dict]:
        """Merge all candidates and deduplicate by URL or synthetic key."""

        all_results = []
        all_results.extend(primary_results or [])
        all_results.extend(general_results or [])
        all_results.extend(internal_results or [])

        url_to_best: dict[str, dict] = {}
        for r in all_results:
            url = r.get("url") or f"internal://{id(r)}"
            score = float(r.get("score", 0.0))
            if url not in url_to_best or score > url_to_best[url]["score"]:
                url_to_best[url] = r

        # sort by vector score first; cross-encoder will do final sort
        deduped = sorted(url_to_best.values(), key=lambda x: x.get("score", 0.0), reverse=True)
        return deduped
